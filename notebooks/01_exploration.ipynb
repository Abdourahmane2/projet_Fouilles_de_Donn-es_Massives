{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üìä Notebook 01 - Exploration des Donn√©es\n",
    "\n",
    "## Projet : D√©tection de Fraudes - Transactions par Ch√®que\n",
    "\n",
    "**M2 SISE - Fouille de Donn√©es Massives**\n",
    "\n",
    "---\n",
    "\n",
    "### Objectifs de ce notebook\n",
    "\n",
    "1. Charger et valider les donn√©es\n",
    "2. Analyser les statistiques descriptives\n",
    "3. Visualiser les distributions\n",
    "4. Analyser le d√©s√©quilibre des classes\n",
    "5. Identifier les corr√©lations et patterns\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Configuration et imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports standards\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "\n",
    "# Configuration\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.float_format', lambda x: '%.3f' % x)\n",
    "\n",
    "# Style des graphiques\n",
    "plt.style.use('seaborn-v0_8-whitegrid')\n",
    "sns.set_palette('viridis')\n",
    "\n",
    "# Taille par d√©faut des figures\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "plt.rcParams['font.size'] = 12\n",
    "\n",
    "print(\"‚úÖ Imports r√©ussis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import de notre module personnalis√©\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "from config.config import (\n",
    "    RAW_DATA_DIR, DATA_FILENAME, TARGET_COLUMN, \n",
    "    COLUMNS_TO_EXCLUDE, RANDOM_STATE, FIGURES_DIR\n",
    ")\n",
    "from src.data_loader import (\n",
    "    load_raw_data, validate_columns, convert_data_types,\n",
    "    get_data_summary, split_by_date, print_data_summary\n",
    ")\n",
    "\n",
    "print(f\"üìÅ Dossier des donn√©es: {RAW_DATA_DIR}\")\n",
    "print(f\"üìÅ Dossier des figures: {FIGURES_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Chargement des donn√©es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chargement des donn√©es brutes\n",
    "# IMPORTANT: Modifiez DATA_FILENAME dans config/config.py si n√©cessaire\n",
    "\n",
    "df = load_raw_data(verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu des premi√®res lignes\n",
    "print(\"\\nüìã Aper√ßu des donn√©es (5 premi√®res lignes):\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aper√ßu des derni√®res lignes\n",
    "print(\"\\nüìã Aper√ßu des donn√©es (5 derni√®res lignes):\")\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validation des colonnes\n",
    "validation = validate_columns(df)\n",
    "\n",
    "print(\"\\nüîç Validation des colonnes:\")\n",
    "print(f\"   - Valide: {validation['valid']}\")\n",
    "print(f\"   - Colonnes attendues: {validation['expected_count']}\")\n",
    "print(f\"   - Colonnes trouv√©es: {validation['actual_count']}\")\n",
    "\n",
    "if validation['missing_columns']:\n",
    "    print(f\"   ‚ö†Ô∏è Colonnes manquantes: {validation['missing_columns']}\")\n",
    "if validation['extra_columns']:\n",
    "    print(f\"   ‚ÑπÔ∏è Colonnes suppl√©mentaires: {validation['extra_columns']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des types\n",
    "df = convert_data_types(df, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Statistiques descriptives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# R√©sum√© global\n",
    "summary = get_data_summary(df)\n",
    "print_data_summary(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information sur les types de donn√©es\n",
    "print(\"\\nüìä Types de donn√©es:\")\n",
    "print(df.dtypes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques descriptives pour les variables num√©riques\n",
    "print(\"\\nüìà Statistiques descriptives (variables num√©riques):\")\n",
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Valeurs manquantes d√©taill√©es\n",
    "missing = df.isnull().sum()\n",
    "missing_pct = (missing / len(df) * 100).round(2)\n",
    "\n",
    "missing_df = pd.DataFrame({\n",
    "    'Valeurs manquantes': missing,\n",
    "    'Pourcentage (%)': missing_pct\n",
    "}).sort_values('Valeurs manquantes', ascending=False)\n",
    "\n",
    "print(\"\\n‚ùì Valeurs manquantes par colonne:\")\n",
    "missing_df[missing_df['Valeurs manquantes'] > 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analyse du d√©s√©quilibre des classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution de la variable cible\n",
    "target_counts = df[TARGET_COLUMN].value_counts()\n",
    "target_pct = df[TARGET_COLUMN].value_counts(normalize=True) * 100\n",
    "\n",
    "print(f\"\\nüéØ Distribution de la variable cible '{TARGET_COLUMN}':\")\n",
    "print(\"\\n   Valeur | Count      | Pourcentage\")\n",
    "print(\"   \" + \"-\" * 40)\n",
    "for val in target_counts.index:\n",
    "    label = \"Normal\" if val == 0 else \"Fraude\"\n",
    "    print(f\"   {val} ({label:6s}) | {target_counts[val]:>10,} | {target_pct[val]:>6.2f}%\")\n",
    "\n",
    "imbalance_ratio = target_counts.min() / target_counts.max()\n",
    "print(f\"\\n   üìä Ratio de d√©s√©quilibre: {imbalance_ratio:.4f} (1:{1/imbalance_ratio:.0f})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation du d√©s√©quilibre\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Graphique en barres\n",
    "colors = ['#2ecc71', '#e74c3c']\n",
    "bars = axes[0].bar(['Normal (0)', 'Fraude (1)'], target_counts.values, color=colors, edgecolor='black')\n",
    "axes[0].set_ylabel('Nombre de transactions', fontsize=12)\n",
    "axes[0].set_title('Distribution de la variable cible', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Ajouter les valeurs sur les barres\n",
    "for bar, count, pct in zip(bars, target_counts.values, target_pct.values):\n",
    "    axes[0].text(bar.get_x() + bar.get_width()/2, bar.get_height() + 1000, \n",
    "                 f'{count:,}\\n({pct:.2f}%)', ha='center', va='bottom', fontsize=11)\n",
    "\n",
    "# Graphique en camembert\n",
    "axes[1].pie(target_counts.values, labels=['Normal (0)', 'Fraude (1)'], \n",
    "            autopct='%1.2f%%', colors=colors, explode=[0, 0.1],\n",
    "            shadow=True, startangle=90)\n",
    "axes[1].set_title('Proportion des classes', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'class_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüíæ Figure sauvegard√©e: {FIGURES_DIR / 'class_distribution.png'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Analyse des variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Liste des variables num√©riques (excluant les identifiants et la date)\n",
    "exclude_cols = ['ZIBZIN', 'IDAvisAutorisationCheque', 'DateTransaction', 'CodeDecision']\n",
    "numeric_cols = [col for col in df.select_dtypes(include=[np.number]).columns \n",
    "                if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nüìä Variables num√©riques √† analyser ({len(numeric_cols)}):\")\n",
    "for i, col in enumerate(numeric_cols, 1):\n",
    "    print(f\"   {i:2d}. {col}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Distribution du montant des transactions\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Histogramme global\n",
    "axes[0, 0].hist(df['Montant'], bins=50, edgecolor='black', alpha=0.7)\n",
    "axes[0, 0].set_xlabel('Montant (‚Ç¨)')\n",
    "axes[0, 0].set_ylabel('Fr√©quence')\n",
    "axes[0, 0].set_title('Distribution des montants (tous)')\n",
    "axes[0, 0].axvline(df['Montant'].median(), color='red', linestyle='--', label=f\"M√©diane: {df['Montant'].median():.0f}‚Ç¨\")\n",
    "axes[0, 0].legend()\n",
    "\n",
    "# Histogramme par classe\n",
    "df[df[TARGET_COLUMN] == 0]['Montant'].hist(ax=axes[0, 1], bins=50, alpha=0.6, label='Normal', color='green')\n",
    "df[df[TARGET_COLUMN] == 1]['Montant'].hist(ax=axes[0, 1], bins=50, alpha=0.6, label='Fraude', color='red')\n",
    "axes[0, 1].set_xlabel('Montant (‚Ç¨)')\n",
    "axes[0, 1].set_ylabel('Fr√©quence')\n",
    "axes[0, 1].set_title('Distribution des montants par classe')\n",
    "axes[0, 1].legend()\n",
    "\n",
    "# Boxplot par classe\n",
    "df.boxplot(column='Montant', by=TARGET_COLUMN, ax=axes[1, 0])\n",
    "axes[1, 0].set_xlabel('Classe')\n",
    "axes[1, 0].set_ylabel('Montant (‚Ç¨)')\n",
    "axes[1, 0].set_title('Boxplot des montants par classe')\n",
    "plt.suptitle('')\n",
    "\n",
    "# Log-scale pour mieux voir\n",
    "axes[1, 1].hist(df['Montant'], bins=50, edgecolor='black', alpha=0.7, log=True)\n",
    "axes[1, 1].set_xlabel('Montant (‚Ç¨)')\n",
    "axes[1, 1].set_ylabel('Fr√©quence (log)')\n",
    "axes[1, 1].set_title('Distribution des montants (√©chelle log)')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'montant_distribution.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques du montant par classe\n",
    "print(\"\\nüí∞ Statistiques du montant par classe:\")\n",
    "df.groupby(TARGET_COLUMN)['Montant'].describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyse de la variable CodeDecision\n",
    "print(\"\\nüîç Distribution de CodeDecision:\")\n",
    "code_counts = df['CodeDecision'].value_counts().sort_index()\n",
    "print(code_counts)\n",
    "\n",
    "# Crosstab avec la cible\n",
    "print(\"\\nüìä Crosstab CodeDecision vs FlagImpaye:\")\n",
    "ct = pd.crosstab(df['CodeDecision'], df[TARGET_COLUMN], margins=True, margins_name='Total')\n",
    "ct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Taux de fraude par CodeDecision\n",
    "fraud_rate_by_code = df.groupby('CodeDecision')[TARGET_COLUMN].mean() * 100\n",
    "\n",
    "print(\"\\nüìà Taux de fraude par CodeDecision:\")\n",
    "for code, rate in fraud_rate_by_code.items():\n",
    "    print(f\"   Code {code}: {rate:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Analyse temporelle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraction des composantes temporelles\n",
    "df['Date'] = df['DateTransaction'].dt.date\n",
    "df['Month'] = df['DateTransaction'].dt.month\n",
    "df['DayOfWeek'] = df['DateTransaction'].dt.dayofweek\n",
    "df['Hour'] = df['DateTransaction'].dt.hour\n",
    "\n",
    "# Transactions par mois\n",
    "monthly_stats = df.groupby('Month').agg({\n",
    "    TARGET_COLUMN: ['count', 'sum', 'mean']\n",
    "}).round(4)\n",
    "monthly_stats.columns = ['Total', 'Fraudes', 'Taux_fraude']\n",
    "\n",
    "print(\"\\nüìÖ Statistiques mensuelles:\")\n",
    "monthly_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation temporelle\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Transactions par mois\n",
    "monthly_stats['Total'].plot(kind='bar', ax=axes[0, 0], color='steelblue', edgecolor='black')\n",
    "axes[0, 0].set_xlabel('Mois')\n",
    "axes[0, 0].set_ylabel('Nombre de transactions')\n",
    "axes[0, 0].set_title('Transactions par mois')\n",
    "axes[0, 0].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Taux de fraude par mois\n",
    "(monthly_stats['Taux_fraude'] * 100).plot(kind='bar', ax=axes[0, 1], color='coral', edgecolor='black')\n",
    "axes[0, 1].set_xlabel('Mois')\n",
    "axes[0, 1].set_ylabel('Taux de fraude (%)')\n",
    "axes[0, 1].set_title('Taux de fraude par mois')\n",
    "axes[0, 1].tick_params(axis='x', rotation=0)\n",
    "\n",
    "# Transactions par jour de la semaine\n",
    "dow_stats = df.groupby('DayOfWeek')[TARGET_COLUMN].agg(['count', 'mean'])\n",
    "days = ['Lun', 'Mar', 'Mer', 'Jeu', 'Ven', 'Sam', 'Dim']\n",
    "axes[1, 0].bar(days, dow_stats['count'], color='steelblue', edgecolor='black')\n",
    "axes[1, 0].set_xlabel('Jour de la semaine')\n",
    "axes[1, 0].set_ylabel('Nombre de transactions')\n",
    "axes[1, 0].set_title('Transactions par jour de la semaine')\n",
    "\n",
    "# Taux de fraude par heure\n",
    "hourly_fraud = df.groupby('Hour')[TARGET_COLUMN].mean() * 100\n",
    "axes[1, 1].plot(hourly_fraud.index, hourly_fraud.values, marker='o', color='coral')\n",
    "axes[1, 1].fill_between(hourly_fraud.index, hourly_fraud.values, alpha=0.3, color='coral')\n",
    "axes[1, 1].set_xlabel('Heure')\n",
    "axes[1, 1].set_ylabel('Taux de fraude (%)')\n",
    "axes[1, 1].set_title('Taux de fraude par heure')\n",
    "axes[1, 1].set_xlim(0, 23)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'temporal_analysis.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Analyse des corr√©lations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Matrice de corr√©lation\n",
    "corr_cols = numeric_cols + [TARGET_COLUMN] if TARGET_COLUMN not in numeric_cols else numeric_cols\n",
    "corr_matrix = df[corr_cols].corr()\n",
    "\n",
    "# Heatmap\n",
    "plt.figure(figsize=(16, 14))\n",
    "mask = np.triu(np.ones_like(corr_matrix, dtype=bool))\n",
    "sns.heatmap(corr_matrix, mask=mask, annot=True, cmap='RdBu_r', center=0,\n",
    "            fmt='.2f', square=True, linewidths=0.5,\n",
    "            cbar_kws={'shrink': 0.8})\n",
    "plt.title('Matrice de corr√©lation', fontsize=16, fontweight='bold')\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'correlation_matrix.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Corr√©lations avec la variable cible\n",
    "target_corr = corr_matrix[TARGET_COLUMN].drop(TARGET_COLUMN).sort_values(key=abs, ascending=False)\n",
    "\n",
    "print(f\"\\nüéØ Corr√©lations avec {TARGET_COLUMN} (tri√©es par valeur absolue):\")\n",
    "print(\"\\n   Variable               | Corr√©lation\")\n",
    "print(\"   \" + \"-\" * 40)\n",
    "for var, corr in target_corr.items():\n",
    "    print(f\"   {var:25s} | {corr:+.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualisation des corr√©lations avec la cible\n",
    "fig, ax = plt.subplots(figsize=(12, 8))\n",
    "\n",
    "colors = ['#e74c3c' if x < 0 else '#2ecc71' for x in target_corr.values]\n",
    "bars = ax.barh(target_corr.index, target_corr.values, color=colors, edgecolor='black')\n",
    "ax.axvline(x=0, color='black', linewidth=0.8)\n",
    "ax.set_xlabel('Corr√©lation avec FlagImpaye', fontsize=12)\n",
    "ax.set_title('Corr√©lations des variables avec la variable cible', fontsize=14, fontweight='bold')\n",
    "\n",
    "# Ajouter les valeurs\n",
    "for bar, val in zip(bars, target_corr.values):\n",
    "    x_pos = val + 0.01 if val >= 0 else val - 0.01\n",
    "    ha = 'left' if val >= 0 else 'right'\n",
    "    ax.text(x_pos, bar.get_y() + bar.get_height()/2, f'{val:.3f}', \n",
    "            va='center', ha=ha, fontsize=9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(FIGURES_DIR / 'target_correlations.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. S√©paration Train/Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S√©paration temporelle selon le sujet\n",
    "df_train, df_test = split_by_date(df, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# V√©rification de la coh√©rence\n",
    "print(\"\\n‚úÖ V√©rification de la s√©paration:\")\n",
    "print(f\"   Train - Date min: {df_train['DateTransaction'].min()}\")\n",
    "print(f\"   Train - Date max: {df_train['DateTransaction'].max()}\")\n",
    "print(f\"   Test  - Date min: {df_test['DateTransaction'].min()}\")\n",
    "print(f\"   Test  - Date max: {df_test['DateTransaction'].max()}\")\n",
    "\n",
    "# V√©rifier qu'il n'y a pas de chevauchement\n",
    "assert df_train['DateTransaction'].max() < df_test['DateTransaction'].min(), \"‚ö†Ô∏è Chevauchement d√©tect√©!\"\n",
    "print(\"\\n‚úÖ Aucun chevauchement entre train et test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. R√©sum√© et conclusions de l'exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"R√âSUM√â DE L'EXPLORATION DES DONN√âES\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(f\"\\nüìä DONN√âES:\")\n",
    "print(f\"   - Total: {len(df):,} transactions\")\n",
    "print(f\"   - Variables: {len(df.columns)} colonnes\")\n",
    "print(f\"   - P√©riode: {df['DateTransaction'].min().date()} ‚Üí {df['DateTransaction'].max().date()}\")\n",
    "\n",
    "print(f\"\\nüéØ D√âS√âQUILIBRE DES CLASSES:\")\n",
    "fraud_rate = df[TARGET_COLUMN].mean() * 100\n",
    "print(f\"   - Taux de fraude global: {fraud_rate:.2f}%\")\n",
    "print(f\"   - Ratio: 1:{1/df[TARGET_COLUMN].mean():.0f}\")\n",
    "print(f\"   ‚Üí FORT D√âS√âQUILIBRE - n√©cessite techniques sp√©cifiques\")\n",
    "\n",
    "print(f\"\\nüìÖ S√âPARATION TRAIN/TEST:\")\n",
    "print(f\"   - Train: {len(df_train):,} ({len(df_train)/len(df)*100:.1f}%)\")\n",
    "print(f\"   - Test: {len(df_test):,} ({len(df_test)/len(df)*100:.1f}%)\")\n",
    "\n",
    "print(f\"\\nüîó VARIABLES LES PLUS CORR√âL√âES AVEC LA FRAUDE:\")\n",
    "for var, corr in target_corr.head(5).items():\n",
    "    print(f\"   - {var}: {corr:+.4f}\")\n",
    "\n",
    "print(f\"\\n‚ö†Ô∏è POINTS D'ATTENTION:\")\n",
    "print(f\"   1. Ne PAS utiliser 'CodeDecision' pour la pr√©diction\")\n",
    "print(f\"   2. G√©rer le d√©s√©quilibre (SMOTE, class_weight, etc.)\")\n",
    "print(f\"   3. Optimiser la F-mesure, pas l'accuracy\")\n",
    "print(f\"   4. Respecter la s√©paration temporelle (pas de data leakage)\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 70)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarde des donn√©es pr√©par√©es pour le prochain notebook\n",
    "from config.config import PROCESSED_DATA_DIR\n",
    "\n",
    "df_train.to_pickle(PROCESSED_DATA_DIR / 'df_train.pkl')\n",
    "df_test.to_pickle(PROCESSED_DATA_DIR / 'df_test.pkl')\n",
    "\n",
    "print(f\"\\nüíæ Donn√©es sauvegard√©es:\")\n",
    "print(f\"   - {PROCESSED_DATA_DIR / 'df_train.pkl'}\")\n",
    "print(f\"   - {PROCESSED_DATA_DIR / 'df_test.pkl'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ‚û°Ô∏è Prochaine √©tape\n",
    "\n",
    "Passez au notebook **02_preprocessing.ipynb** pour:\n",
    "- Pr√©paration des features\n",
    "- Gestion des valeurs manquantes\n",
    "- Encodage des variables\n",
    "- Normalisation/Standardisation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
